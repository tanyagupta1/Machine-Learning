{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MH_data_script.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanyagupta1/Machine-Learning/blob/main/BTP/MH_data_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-QQANye3ecE"
      },
      "source": [
        "# Using Pushshift Module to extract Submissions Data from Reddit via Python\n",
        "\n",
        "PRAW is pretty good at gettin reddit data but there are some limitations with it.\n",
        "Including the removal of the [subreddit.submissions endpoint](https://www.reddit.com/r/changelog/comments/7tus5f/update_to_search_api/.). \n",
        "\n",
        "So for extracting Reddit submissions and the primarily data such as upvotes and comments count, I put together this notebook using Pushshift.\n",
        "\n",
        "If you still prefer PRAW for extract submissions, I have written a code [template here](https://github.com/SeyiAgboola/Seyi_Projects/blob/master/submission_list.py).\n",
        "\n",
        "I will also [host the code on GitHub](https://github.com/SeyiAgboola/Reddit-Data-Mining/blob/master/Using_Pushshift_Module_to_extract_Submissions.ipynb).\n",
        "\n",
        "More info on the removal of the [subreddit.submissions endpoint](https://www.reddit.com/r/redditdev/comments/8bia9n/praw_psa_the_subredditsubmissions_method_no/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyarnetivJPG"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfmIce345UaB"
      },
      "source": [
        "import pandas as pd\n",
        "import requests #Pushshift accesses Reddit via an url so this is needed\n",
        "import json #JSON manipulation\n",
        "import csv #To Convert final table into a csv file to save to your machine\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDma-H_k0frf"
      },
      "source": [
        "#Adapted from this https://gist.github.com/dylankilkenny/3dbf6123527260165f8c5c3bc3ee331b\n",
        "#This function builds an Pushshift URL, accesses the webpage and stores JSON data in a nested list\n",
        "def getPushshiftData(after, before, sub):\n",
        "    #Build URL\n",
        "    url = 'https://api.pushshift.io/reddit/search/submission/?size=100&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
        "    #Print URL to show user\n",
        "    print(url)\n",
        "    #Request URL\n",
        "    r = requests.get(url)\n",
        "    #Load JSON data from webpage into data variable\n",
        "    data = json.loads(r.text)\n",
        "    #return the data element which contains all the submissions data\n",
        "    return data['data']\n",
        "\n",
        "def getPushshiftData_comments(sub_id):\n",
        "    #Build URL\n",
        "    url = 'https://api.pushshift.io/reddit/search/comment/?link_id=t3_'+str(sub_id)\n",
        "    #Print URL to show user\n",
        "    print(url)\n",
        "    #Request URL\n",
        "    r = requests.get(url)\n",
        "    #Load JSON data from webpage into data variable\n",
        "    data = json.loads(r.text)\n",
        "    #return the data element which contains all the submissions data\n",
        "    return data['data']"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H524E6kkvh1O"
      },
      "source": [
        "# Extract key information from Submissions\n",
        "* Submission Title\n",
        "* Body of Post \n",
        "* Author\n",
        "* Submission post ID\n",
        "* Score\n",
        "* Awards\n",
        "* Upload Time\n",
        "* No. of Comments \n",
        "\n",
        "# Extract key information from Comments\n",
        "* body,\n",
        "* author\n",
        "* is_op\n",
        "* date created\n",
        "* score\n",
        "* no of awards\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6qX7glQ1-nl"
      },
      "source": [
        "#This function will be used to extract the key data points from each JSON result\n",
        "def collectSubData(subm):\n",
        "    subData = list() \n",
        "    title = subm['title']\n",
        "    body = subm['selftext']\n",
        "    # #flairs are not always present so we wrap in try/except\n",
        "    # try:\n",
        "    #     flair = subm['link_flair_text']\n",
        "    # except KeyError:\n",
        "    #     flair = \"NaN\"    \n",
        "    author = subm['author']\n",
        "    sub_id = subm['id']\n",
        "    score = subm['score']\n",
        "    awards = subm['total_awards_received']\n",
        "    created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
        "    numComms = subm['num_comments']\n",
        "    subData.append((sub_id,title,body,author,score,awards,numComms,created))\n",
        "    subStats[sub_id] = subData\n",
        "\n",
        "def collectComments(subm):\n",
        "    data = getPushshiftData_comments(subm)\n",
        "    comments[subm]=list()\n",
        "    for comment in data:\n",
        "      author=comment['author']\n",
        "      body=comment['body']\n",
        "      created = datetime.datetime.fromtimestamp(comment['created_utc'])\n",
        "      is_op=comment['is_submitter']\n",
        "      score=comment['score']\n",
        "      awards=comment['total_awards_received']\n",
        "      comments[subm].append((body,author,is_op,created,score,awards))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCDRrn9nwRsj"
      },
      "source": [
        "# Update your Search Settings here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0fNU7eS2mwT"
      },
      "source": [
        "#Create your timestamps and queries for your search URL\n",
        "#https://www.unixtimestamp.com/index.php > Use this to create your timestamps\n",
        "after = \"1577817000\" #Submissions after this timestamp - 1 jan 2020 00;00;00\n",
        "before = \"1580322600\" #Submissions before this timestamp -30 jan 2020 00:00:00\n",
        "sub = \"ptsd\" #Which Subreddit to search in\n",
        "#subCount tracks the no. of total submissions we collect\n",
        "subCount = 0\n",
        "no_of_subs_to_collect=100\n",
        "#subStats is the dictionary where we will store our data.\n",
        "subStats = {}\n",
        "comments={}"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wP_j8pp2-M8"
      },
      "source": [
        "# We need to run this function outside the loop first to get the updated after variable\n",
        "data = getPushshiftData(after, before, sub)\n",
        "# Will run until all posts have been gathered i.e. When the length of data variable = 0\n",
        "# from the 'after' date up until before date\n",
        "while len(data) > 0: #The length of data is the number submissions (data[0], data[1] etc), once it hits zero (after and before vars are the same) end\n",
        "    if(subCount>=no_of_subs_to_collect):\n",
        "      break;\n",
        "    for submission in data:\n",
        "        collectSubData(submission)\n",
        "        collectComments(submission['id'])\n",
        "        subCount+=1\n",
        "    # Calls getPushshiftData() with the created date of the last submission\n",
        "    print(len(data))\n",
        "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
        "    #update after variable to last created date of submission\n",
        "    after = data[-1]['created_utc']\n",
        "    #data has changed due to the new after variable provided by above code\n",
        "    data = getPushshiftData(after, before, sub)\n",
        "    \n",
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaeTGu7mwyoU"
      },
      "source": [
        "# Check your Submission Extraction was successful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVLuSJ8e8p7i"
      },
      "source": [
        "# print(str(len(subStats)) + \" submissions have added to list\")\n",
        "# print(\"1st entry is:\")\n",
        "# print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
        "# print(\"Last entry is:\")\n",
        "# print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAm_zZZfw521"
      },
      "source": [
        "# Save data to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBikywNJ8ufl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68c6d046-5512-49b9-abf6-e0e6db351b19"
      },
      "source": [
        "def updateSubs_file():\n",
        "    upload_count = 0\n",
        "    #location = \"\\\\Reddit Data\\\\\" >> If you're running this outside of a notebook you'll need this to direct to a specific location\n",
        "    print(\"input filename of submission file, please add .csv\")\n",
        "    filename = input() #This asks the user what to name the file\n",
        "    file = filename\n",
        "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
        "        a = csv.writer(file, delimiter=',')\n",
        "        headers = ['sub_id','title','body','author','score','awards','numComms','created']\n",
        "        a.writerow(headers)\n",
        "        for sub in subStats:\n",
        "            a.writerow(subStats[sub][0])\n",
        "            upload_count+=1\n",
        "            \n",
        "        print(str(upload_count) + \" submissions have been uploaded\")\n",
        "updateSubs_file()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input filename of submission file, please add .csv\n",
            "ptsd_submissions.csv\n",
            "100 submissions have been uploaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HthTXY5Dam7G",
        "outputId": "ffa1aa0b-b1ae-4232-c14f-735d6a651337"
      },
      "source": [
        "def updateSubs_file_comm():\n",
        "    upload_count = 0\n",
        "    #location = \"\\\\Reddit Data\\\\\" >> If you're running this outside of a notebook you'll need this to direct to a specific location\n",
        "    print(\"input filename of comments file, please add .csv\")\n",
        "    filename = input() #This asks the user what to name the file\n",
        "    file = filename\n",
        "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
        "        fieldnames = ['post_id','body','author','is_op','created','score','awards']\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(fieldnames)\n",
        "        for post in comments:\n",
        "          for com in comments[post]:\n",
        "            tmp=list()\n",
        "            tmp.append(post)\n",
        "            tmp.extend(com)\n",
        "            # print(tmp)\n",
        "            writer.writerow(tmp)\n",
        "            upload_count+=1\n",
        "            \n",
        "        print(str(upload_count) + \" comments have been uploaded\")\n",
        "updateSubs_file_comm()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input filename of comments file, please add .csv\n",
            "ptsd_comments.csv\n",
            "598 comments have been uploaded\n"
          ]
        }
      ]
    }
  ]
}