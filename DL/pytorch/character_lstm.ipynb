{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "character_lstm.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3kLz5xkLroGZZgVMzcFW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanyagupta1/Machine-Learning/blob/main/DL/pytorch/character_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ormg1xwmgbrq"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCPpeNQREnIg",
        "outputId": "7b725cfa-1d3e-40b6-aa69-7d2188161231"
      },
      "source": [
        "with open('/content/anna.txt','r') as f:\n",
        "  text=f.read()\n",
        "print(text[0:100])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chapter 1\n",
            "\n",
            "\n",
            "Happy families are all alike; every unhappy family is unhappy in its own\n",
            "way.\n",
            "\n",
            "Everythin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cy5y2TIFCgw"
      },
      "source": [
        "chars=tuple(set(text))\n",
        "int_to_char =dict(enumerate(chars))\n",
        "char_to_int = {ch:i for i,ch in int_to_char.items()}\n",
        "encode=np.array([char_to_int[c] for c in text])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgLVObTuFx0p",
        "outputId": "5f6353ef-597f-4ae6-8c58-e74e37582391"
      },
      "source": [
        "print(int_to_char)\n",
        "print(encode[0:100])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'q', 1: 'T', 2: '!', 3: 'X', 4: '/', 5: 'N', 6: '7', 7: '\\n', 8: '6', 9: ';', 10: 'D', 11: 'I', 12: 'R', 13: 'C', 14: 'j', 15: 'G', 16: 'a', 17: ' ', 18: 'V', 19: 'K', 20: 'y', 21: 'W', 22: '*', 23: 'M', 24: '(', 25: \"'\", 26: '5', 27: 'F', 28: 'z', 29: 'm', 30: '2', 31: 'r', 32: '\"', 33: 'l', 34: 'd', 35: ')', 36: '&', 37: 'A', 38: '_', 39: 'J', 40: 'S', 41: 'g', 42: '`', 43: 'b', 44: 'e', 45: 'p', 46: 's', 47: 'Z', 48: '9', 49: '.', 50: '-', 51: ',', 52: 'U', 53: 't', 54: '?', 55: '$', 56: 'n', 57: 'o', 58: 'h', 59: '3', 60: 'x', 61: 'P', 62: '1', 63: 'E', 64: 'w', 65: 'Q', 66: 'Y', 67: 'c', 68: ':', 69: 'H', 70: '0', 71: 'B', 72: 'L', 73: '%', 74: 'O', 75: '4', 76: 'k', 77: '@', 78: 'i', 79: 'v', 80: 'f', 81: '8', 82: 'u'}\n",
            "[13 58 16 45 53 44 31 17 62  7  7  7 69 16 45 45 20 17 80 16 29 78 33 78\n",
            " 44 46 17 16 31 44 17 16 33 33 17 16 33 78 76 44  9 17 44 79 44 31 20 17\n",
            " 82 56 58 16 45 45 20 17 80 16 29 78 33 20 17 78 46 17 82 56 58 16 45 45\n",
            " 20 17 78 56 17 78 53 46 17 57 64 56  7 64 16 20 49  7  7 63 79 44 31 20\n",
            " 53 58 78 56]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3_jCZskHE_W"
      },
      "source": [
        "def one_hot_encode(arr,n_labels):\n",
        "\n",
        "  one_hot=np.zeros((arr.size,n_labels),dtype=np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]),arr.flatten()] = 1\n",
        "  one_hot = one_hot.reshape((*arr.shape,n_labels))\n",
        "  return one_hot"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EFvGzEzHaaU",
        "outputId": "b914077c-bb2c-4e38-bf7e-ef1e35603818"
      },
      "source": [
        "test_seq = np.array([3,5,1])\n",
        "one_hot = one_hot_encode(test_seq,8)\n",
        "print(one_hot)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRM97fHyOxYV"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    batch_size_total = seq_length * batch_size\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    \n",
        "    arr = arr.reshape((batch_size,-1))\n",
        "    print(arr.shape)\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "      \n",
        "        x = arr[:,n:n+seq_length]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "          y[:,:-1] , y[:,-1] = x[:,1:] , arr[:,n+seq_length]\n",
        "        except IndexError:\n",
        "          y[:,:-1] , y[:,-1] = x[:,1:] , arr[:,0]\n",
        "        yield x, y"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPxDZigNq1y6",
        "outputId": "97dac373-f203-45c1-c181-2d5648b95673"
      },
      "source": [
        "batches = get_batches(encode,8,50)\n",
        "x,y = next(batches)\n",
        "print(x[0])\n",
        "print(y[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 248150)\n",
            "[13 58 16 45 53 44 31 17 62  7  7  7 69 16 45 45 20 17 80 16 29 78 33 78\n",
            " 44 46 17 16 31 44 17 16 33 33 17 16 33 78 76 44  9 17 44 79 44 31 20 17\n",
            " 82 56]\n",
            "[58 16 45 53 44 31 17 62  7  7  7 69 16 45 45 20 17 80 16 29 78 33 78 44\n",
            " 46 17 16 31 44 17 16 33 33 17 16 33 78 76 44  9 17 44 79 44 31 20 17 82\n",
            " 56 58]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiG8DsoMQv-E"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "  def __init__(self,tokens,n_hidden=256,n_layers=2,drop_prob=0.5,lr=0.001):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_layers = n_layers\n",
        "    self.lr = lr\n",
        "    self.chars = tokens\n",
        "    self.int_to_char =dict(enumerate(self.chars))\n",
        "    self.char_to_int={ch:i for i,ch in self.int_to_char.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.chars),n_hidden,n_layers,dropout=drop_prob,batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc = nn.Linear(n_hidden,len(self.chars))\n",
        "\n",
        "  def forward(self,x,hidden):\n",
        "    r_output,hidden =self.lstm(x,hidden)\n",
        "    out = self.dropout(r_output)\n",
        "    out = out.contiguous().view(-1,self.n_hidden)\n",
        "    out = self.fc(out)\n",
        "    return out,hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "\n",
        "    weight = next(self.parameters()).data\n",
        "\n",
        "   \n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    \n",
        "    return hidden"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDzWM-fmWgOt"
      },
      "source": [
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "  \n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVWR1_ntztI6",
        "outputId": "1d92678c-ab75-48bc-8b39-2656f9e62df0"
      },
      "source": [
        "net = CharRNN(chars, 512, 2)\n",
        "print(net)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ry8unOZg0lQN",
        "outputId": "26e517dc-4e16-4cbb-8067-41f4ca30cd4d"
      },
      "source": [
        "batch_size = 128\n",
        "seq_length = 100\n",
        "n_epochs = 20\n",
        "\n",
        "# train the model\n",
        "train(net, encode, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 10... Loss: 3.2600... Val Loss: 3.1989\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1484... Val Loss: 3.1332\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1426... Val Loss: 3.1234\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1116... Val Loss: 3.1184\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1398... Val Loss: 3.1166\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1181... Val Loss: 3.1134\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1041... Val Loss: 3.1080\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1125... Val Loss: 3.0995\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 90... Loss: 3.0921... Val Loss: 3.0753\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 100... Loss: 3.0326... Val Loss: 3.0167\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 110... Loss: 2.9656... Val Loss: 2.9455\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 120... Loss: 2.8341... Val Loss: 2.8203\n",
            "(128, 1500)\n",
            "Epoch: 1/20... Step: 130... Loss: 2.7855... Val Loss: 2.8155\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 140... Loss: 2.7253... Val Loss: 2.6830\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 150... Loss: 2.6255... Val Loss: 2.5721\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 160... Loss: 2.5437... Val Loss: 2.5034\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 170... Loss: 2.4803... Val Loss: 2.4569\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 180... Loss: 2.4487... Val Loss: 2.4191\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 190... Loss: 2.3946... Val Loss: 2.3832\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 200... Loss: 2.3910... Val Loss: 2.3799\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 210... Loss: 2.3570... Val Loss: 2.3261\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 220... Loss: 2.3221... Val Loss: 2.2953\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 230... Loss: 2.3040... Val Loss: 2.2665\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 240... Loss: 2.2743... Val Loss: 2.2410\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 250... Loss: 2.2138... Val Loss: 2.2146\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 260... Loss: 2.1882... Val Loss: 2.1884\n",
            "(128, 1500)\n",
            "Epoch: 2/20... Step: 270... Loss: 2.2097... Val Loss: 2.1676\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 280... Loss: 2.1934... Val Loss: 2.1431\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 290... Loss: 2.1615... Val Loss: 2.1173\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 300... Loss: 2.1384... Val Loss: 2.0993\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 310... Loss: 2.1023... Val Loss: 2.0794\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 320... Loss: 2.0691... Val Loss: 2.0559\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 330... Loss: 2.0416... Val Loss: 2.0398\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 340... Loss: 2.0605... Val Loss: 2.0204\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 350... Loss: 2.0421... Val Loss: 2.0061\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 360... Loss: 1.9823... Val Loss: 1.9855\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 370... Loss: 2.0097... Val Loss: 1.9656\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 380... Loss: 1.9786... Val Loss: 1.9500\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 390... Loss: 1.9543... Val Loss: 1.9349\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 400... Loss: 1.9311... Val Loss: 1.9210\n",
            "(128, 1500)\n",
            "Epoch: 3/20... Step: 410... Loss: 1.9371... Val Loss: 1.9066\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 420... Loss: 1.9290... Val Loss: 1.8920\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 430... Loss: 1.9211... Val Loss: 1.8763\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 440... Loss: 1.8938... Val Loss: 1.8671\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 450... Loss: 1.8398... Val Loss: 1.8515\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 460... Loss: 1.8284... Val Loss: 1.8397\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 470... Loss: 1.8595... Val Loss: 1.8333\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 480... Loss: 1.8346... Val Loss: 1.8165\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 490... Loss: 1.8437... Val Loss: 1.8057\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 500... Loss: 1.8301... Val Loss: 1.7910\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 510... Loss: 1.8143... Val Loss: 1.7835\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 520... Loss: 1.8282... Val Loss: 1.7723\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 530... Loss: 1.7865... Val Loss: 1.7671\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 540... Loss: 1.7492... Val Loss: 1.7550\n",
            "(128, 1500)\n",
            "Epoch: 4/20... Step: 550... Loss: 1.7892... Val Loss: 1.7453\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 560... Loss: 1.7666... Val Loss: 1.7368\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 570... Loss: 1.7561... Val Loss: 1.7243\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 580... Loss: 1.7272... Val Loss: 1.7184\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 590... Loss: 1.7258... Val Loss: 1.7101\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 600... Loss: 1.7232... Val Loss: 1.6986\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 610... Loss: 1.7011... Val Loss: 1.6948\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 620... Loss: 1.7119... Val Loss: 1.6872\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 630... Loss: 1.7159... Val Loss: 1.6840\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 640... Loss: 1.6953... Val Loss: 1.6714\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 650... Loss: 1.6764... Val Loss: 1.6705\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 660... Loss: 1.6614... Val Loss: 1.6603\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 670... Loss: 1.6866... Val Loss: 1.6551\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 680... Loss: 1.6791... Val Loss: 1.6465\n",
            "(128, 1500)\n",
            "Epoch: 5/20... Step: 690... Loss: 1.6563... Val Loss: 1.6431\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 700... Loss: 1.6595... Val Loss: 1.6404\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 710... Loss: 1.6499... Val Loss: 1.6300\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 720... Loss: 1.6382... Val Loss: 1.6238\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 730... Loss: 1.6462... Val Loss: 1.6179\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 740... Loss: 1.6162... Val Loss: 1.6130\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 750... Loss: 1.5900... Val Loss: 1.6091\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 760... Loss: 1.6363... Val Loss: 1.6034\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 770... Loss: 1.6167... Val Loss: 1.6025\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 780... Loss: 1.5998... Val Loss: 1.5946\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 790... Loss: 1.5835... Val Loss: 1.5917\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 800... Loss: 1.5979... Val Loss: 1.5839\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 810... Loss: 1.5846... Val Loss: 1.5802\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 820... Loss: 1.5533... Val Loss: 1.5745\n",
            "(128, 1500)\n",
            "Epoch: 6/20... Step: 830... Loss: 1.6027... Val Loss: 1.5721\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 840... Loss: 1.5564... Val Loss: 1.5691\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 850... Loss: 1.5727... Val Loss: 1.5662\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 860... Loss: 1.5616... Val Loss: 1.5590\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 870... Loss: 1.5647... Val Loss: 1.5513\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 880... Loss: 1.5668... Val Loss: 1.5469\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 890... Loss: 1.5564... Val Loss: 1.5414\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 900... Loss: 1.5386... Val Loss: 1.5374\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 910... Loss: 1.5134... Val Loss: 1.5376\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 920... Loss: 1.5388... Val Loss: 1.5291\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 930... Loss: 1.5189... Val Loss: 1.5299\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 940... Loss: 1.5216... Val Loss: 1.5295\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 950... Loss: 1.5447... Val Loss: 1.5225\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 960... Loss: 1.5310... Val Loss: 1.5192\n",
            "(128, 1500)\n",
            "Epoch: 7/20... Step: 970... Loss: 1.5375... Val Loss: 1.5163\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 980... Loss: 1.5070... Val Loss: 1.5163\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 990... Loss: 1.5109... Val Loss: 1.5111\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1000... Loss: 1.4996... Val Loss: 1.5036\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1010... Loss: 1.5434... Val Loss: 1.4992\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1020... Loss: 1.4999... Val Loss: 1.4990\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1030... Loss: 1.4843... Val Loss: 1.4945\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1040... Loss: 1.5124... Val Loss: 1.4940\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1050... Loss: 1.4733... Val Loss: 1.4909\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1060... Loss: 1.4921... Val Loss: 1.4862\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1070... Loss: 1.4911... Val Loss: 1.4833\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1080... Loss: 1.4821... Val Loss: 1.4819\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1090... Loss: 1.4778... Val Loss: 1.4776\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1100... Loss: 1.4637... Val Loss: 1.4713\n",
            "(128, 1500)\n",
            "Epoch: 8/20... Step: 1110... Loss: 1.4679... Val Loss: 1.4724\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1120... Loss: 1.4754... Val Loss: 1.4738\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1130... Loss: 1.4803... Val Loss: 1.4693\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1140... Loss: 1.4845... Val Loss: 1.4650\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1150... Loss: 1.4889... Val Loss: 1.4640\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1160... Loss: 1.4438... Val Loss: 1.4593\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1170... Loss: 1.4585... Val Loss: 1.4575\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1180... Loss: 1.4447... Val Loss: 1.4559\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1190... Loss: 1.4786... Val Loss: 1.4544\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1200... Loss: 1.4333... Val Loss: 1.4491\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1210... Loss: 1.4438... Val Loss: 1.4470\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1220... Loss: 1.4456... Val Loss: 1.4470\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1230... Loss: 1.4169... Val Loss: 1.4444\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1240... Loss: 1.4274... Val Loss: 1.4396\n",
            "(128, 1500)\n",
            "Epoch: 9/20... Step: 1250... Loss: 1.4367... Val Loss: 1.4383\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1260... Loss: 1.4408... Val Loss: 1.4394\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1270... Loss: 1.4313... Val Loss: 1.4326\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1280... Loss: 1.4496... Val Loss: 1.4297\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1290... Loss: 1.4361... Val Loss: 1.4296\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1300... Loss: 1.4224... Val Loss: 1.4284\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1310... Loss: 1.4291... Val Loss: 1.4293\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1320... Loss: 1.4045... Val Loss: 1.4275\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1330... Loss: 1.4087... Val Loss: 1.4205\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1340... Loss: 1.3935... Val Loss: 1.4204\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1350... Loss: 1.3946... Val Loss: 1.4168\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1360... Loss: 1.3861... Val Loss: 1.4179\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1370... Loss: 1.3839... Val Loss: 1.4150\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1380... Loss: 1.4227... Val Loss: 1.4107\n",
            "(128, 1500)\n",
            "Epoch: 10/20... Step: 1390... Loss: 1.4277... Val Loss: 1.4108\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1400... Loss: 1.4277... Val Loss: 1.4108\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1410... Loss: 1.4410... Val Loss: 1.4097\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1420... Loss: 1.4258... Val Loss: 1.4054\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1430... Loss: 1.3930... Val Loss: 1.4077\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1440... Loss: 1.4207... Val Loss: 1.4020\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1450... Loss: 1.3488... Val Loss: 1.4050\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1460... Loss: 1.3775... Val Loss: 1.4027\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1470... Loss: 1.3670... Val Loss: 1.3994\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1480... Loss: 1.3869... Val Loss: 1.3954\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1490... Loss: 1.3791... Val Loss: 1.3981\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1500... Loss: 1.3751... Val Loss: 1.4003\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1510... Loss: 1.3506... Val Loss: 1.3949\n",
            "(128, 1500)\n",
            "Epoch: 11/20... Step: 1520... Loss: 1.3828... Val Loss: 1.3944\n",
            "(128, 13900)\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1530... Loss: 1.4275... Val Loss: 1.3899\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1540... Loss: 1.3886... Val Loss: 1.3905\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1550... Loss: 1.3953... Val Loss: 1.3904\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1560... Loss: 1.3953... Val Loss: 1.3832\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1570... Loss: 1.3503... Val Loss: 1.3901\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1580... Loss: 1.3326... Val Loss: 1.3847\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1590... Loss: 1.3285... Val Loss: 1.3784\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1600... Loss: 1.3538... Val Loss: 1.3804\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1610... Loss: 1.3452... Val Loss: 1.3800\n",
            "(128, 1500)\n",
            "Epoch: 12/20... Step: 1620... Loss: 1.3379... Val Loss: 1.3734\n",
            "(128, 1500)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-3796afb76ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-c682cef54337>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-4cf0c6efa867>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mr_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 680\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}